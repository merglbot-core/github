name: Merglbot DORA Metrics Collector

on:
  schedule:
    - cron: '0 8 * * 1' # Weekly Monday 08:00 UTC
  workflow_dispatch:
    inputs:
      period_days:
        description: "Period to analyze (days)"
        default: 30
        required: false
        type: number

permissions:
  contents: read
  actions: read
  issues: read
  pull-requests: read

env:
  PERIOD_DAYS: ${{ inputs.period_days || 30 }}

jobs:
  collect-dora:
    name: Collect DORA Metrics
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1 (pinned SHA; https://github.com/actions/checkout/releases/tag/v6.0.1)

      - name: Compute Period
        id: period
        run: |
          set -euo pipefail

          if ! [[ "$PERIOD_DAYS" =~ ^[0-9]+$ ]] || [ "$PERIOD_DAYS" -lt 1 ]; then
            echo "Invalid PERIOD_DAYS: $PERIOD_DAYS (must be a positive integer >= 1)" >&2
            exit 1
          fi

          SINCE_DAY=$(date -u -d "-$PERIOD_DAYS days" '+%Y-%m-%d' 2>/dev/null || date -u -v-${PERIOD_DAYS}d '+%Y-%m-%d')
          SINCE_ISO=$(date -u -d "-$PERIOD_DAYS days" '+%Y-%m-%dT%H:%M:%SZ' 2>/dev/null || date -u -v-${PERIOD_DAYS}d '+%Y-%m-%dT%H:%M:%SZ')

          echo "since_day=$SINCE_DAY" >> "$GITHUB_OUTPUT"
          echo "since_iso=$SINCE_ISO" >> "$GITHUB_OUTPUT"

      # 1) DEPLOYMENT FREQUENCY (heuristic)
      - name: Calculate Deployment Frequency
        id: deploy_freq
        env:
          GH_TOKEN: ${{ github.token }}
          SINCE_DAY: ${{ steps.period.outputs.since_day }}
        run: |
          set -euo pipefail

          echo "ðŸ“¦ Calculating Deployment Frequency since $SINCE_DAY..."

          # Heuristic: count successful runs of workflows whose name/path contains "deploy".
          WORKFLOW_OUTPUT=$(gh workflow list --all --json name,path,state 2>&1) || {
            echo "::error::Failed to list workflows: $WORKFLOW_OUTPUT"
            exit 1
          }

          mapfile -t DEPLOY_WORKFLOW_PATHS < <(
            echo "$WORKFLOW_OUTPUT" | jq -r '.[] | select(.state == "active") | select((.name | test("deploy"; "i")) or (.path | test("deploy"; "i"))) | .path'
          )
          if [ ${#DEPLOY_WORKFLOW_PATHS[@]} -eq 0 ] || [ -z "${DEPLOY_WORKFLOW_PATHS[0]:-}" ]; then
            echo "No deploy workflows matched; using DEPLOY_COUNT=0."
            DEPLOY_WORKFLOW_PATHS=()
          fi

          DEPLOY_COUNT=0
          for wf_path in "${DEPLOY_WORKFLOW_PATHS[@]}"; do
            [ -z "$wf_path" ] && continue
            COUNT=$(gh run list --workflow="$wf_path" --status=success --created=">=$SINCE_DAY" --limit 200 --json databaseId --jq 'length' 2>/dev/null || echo 0)
            DEPLOY_COUNT=$((DEPLOY_COUNT + COUNT))
          done

          echo "count=$DEPLOY_COUNT" >> "$GITHUB_OUTPUT"

          export DEPLOY_COUNT
          python3 - <<'PY' >> "$GITHUB_OUTPUT"
          import os

          deploy_count = int(os.environ.get("DEPLOY_COUNT", "0") or 0)
          period_days = int(os.environ.get("PERIOD_DAYS", "0") or 0)

          freq = (deploy_count / period_days) if period_days else 0.0

          if freq >= 1:
            classification = "elite"
          elif freq >= 0.14:
            classification = "high"
          elif freq >= 0.03:
            classification = "medium"
          else:
            classification = "low"

          print(f"per_day={freq:.2f}")
          print(f"classification={classification}")
          PY

      # 2) LEAD TIME FOR CHANGES (heuristic: PR createdAt â†’ mergedAt)
      - name: Calculate Lead Time
        id: lead_time
        env:
          GH_TOKEN: ${{ github.token }}
          SINCE_DAY: ${{ steps.period.outputs.since_day }}
        run: |
          set -euo pipefail

          echo "â±ï¸ Calculating Lead Time since $SINCE_DAY..."

          PRS_JSON=$(gh pr list --state merged --search "merged:>=$SINCE_DAY" --limit 500 --json number,createdAt,mergedAt 2>/dev/null || echo "[]")
          if ! echo "$PRS_JSON" | jq empty 2>/dev/null; then
            echo "::warning::Invalid JSON from gh pr list; using empty array"
            PRS_JSON="[]"
          fi

          export PRS_JSON
          python3 - "$GITHUB_OUTPUT" <<'PY'
          import json
          import math
          import statistics
          import os
          import sys
          from datetime import datetime

          out_path = sys.argv[1]
          prs = json.loads(os.environ.get("PRS_JSON", "[]"))

          def parse_iso(ts: str) -> datetime:
            return datetime.fromisoformat(ts.replace("Z", "+00:00"))

          durations = []
          for pr in prs:
            created = pr.get("createdAt")
            merged = pr.get("mergedAt")
            if not created or not merged:
              continue
            durations.append((parse_iso(merged) - parse_iso(created)).total_seconds() / 3600.0)

          durations.sort()
          n = len(durations)

          def pctl(p: float) -> float:
            if n == 0:
              return 0.0
            idx = max(0, min(n - 1, math.ceil((p / 100.0) * n) - 1))
            return durations[idx]

          median = statistics.median(durations) if durations else 0.0
          p95 = pctl(95)

          with open(out_path, "a", encoding="utf-8") as f:
            f.write(f"count={n}\n")
            f.write(f"median_hours={median:.2f}\n")
            f.write(f"p95_hours={p95:.2f}\n")
          PY

      # 3) CHANGE FAILURE RATE (heuristic)
      - name: Calculate Change Failure Rate
        id: cfr
        env:
          GH_TOKEN: ${{ github.token }}
          SINCE_DAY: ${{ steps.period.outputs.since_day }}
          TOTAL_DEPLOYS: ${{ steps.deploy_freq.outputs.count }}
        run: |
          set -euo pipefail

          echo "ðŸ”¥ Calculating Change Failure Rate since $SINCE_DAY..."

          FAILURES=$(gh pr list --state merged --search "merged:>=$SINCE_DAY (hotfix OR revert OR rollback)" --limit 500 --json number --jq 'length' 2>/dev/null || echo 0)

          if [ "${TOTAL_DEPLOYS:-0}" -gt 0 ]; then
            CFR=$(awk "BEGIN {printf \"%.1f\", $FAILURES * 100 / $TOTAL_DEPLOYS}")
          else
            CFR="0"
          fi

          echo "failures=$FAILURES" >> "$GITHUB_OUTPUT"
          echo "percent=$CFR" >> "$GITHUB_OUTPUT"

      # 4) MEAN TIME TO RECOVERY (MTTR) (heuristic: incident issues openâ†’close time)
      - name: Calculate MTTR
        id: mttr
        env:
          GH_TOKEN: ${{ github.token }}
          SINCE_DAY: ${{ steps.period.outputs.since_day }}
        run: |
          set -euo pipefail

          echo "ðŸ› ï¸ Calculating MTTR since $SINCE_DAY..."

          ISSUES_JSON=$(gh issue list --state closed --label incident --search "closed:>=$SINCE_DAY" --limit 500 --json number,createdAt,closedAt 2>/dev/null || echo "[]")
          if ! echo "$ISSUES_JSON" | jq empty 2>/dev/null; then
            echo "::warning::Invalid JSON from gh issue list; using empty array"
            ISSUES_JSON="[]"
          fi

          export ISSUES_JSON
          python3 - "$GITHUB_OUTPUT" <<'PY'
          import json
          import os
          import sys
          from datetime import datetime

          out_path = sys.argv[1]
          issues = json.loads(os.environ.get("ISSUES_JSON", "[]"))

          def parse_iso(ts: str) -> datetime:
            return datetime.fromisoformat(ts.replace("Z", "+00:00"))

          durations = []
          for issue in issues:
            created = issue.get("createdAt")
            closed = issue.get("closedAt")
            if not created or not closed:
              continue
            durations.append((parse_iso(closed) - parse_iso(created)).total_seconds() / 3600.0)

          count = len(durations)
          avg = (sum(durations) / count) if count else 0.0

          with open(out_path, "a", encoding="utf-8") as f:
            f.write(f"count={count}\n")
            f.write(f"hours={avg:.2f}\n")
          PY

      - name: Generate DORA Report
        run: |
          set -euo pipefail

          DEPLOY_PER_DAY="${{ steps.deploy_freq.outputs.per_day }}"
          LEAD_P95="${{ steps.lead_time.outputs.p95_hours }}"
          CFR_PCT="${{ steps.cfr.outputs.percent }}"
          MTTR_HOURS="${{ steps.mttr.outputs.hours }}"

          # Targets
          TARGET_DEPLOY_PER_DAY="1"
          TARGET_LEAD_P95_HOURS="24"
          TARGET_CFR_PCT="10"
          TARGET_MTTR_HOURS="4"

          DEPLOY_PER_DAY="${DEPLOY_PER_DAY:-0}"
          LEAD_P95="${LEAD_P95:-0}"
          CFR_PCT="${CFR_PCT:-0}"
          MTTR_HOURS="${MTTR_HOURS:-0}"

          export DEPLOY_PER_DAY LEAD_P95 CFR_PCT MTTR_HOURS
          export TARGET_DEPLOY_PER_DAY TARGET_LEAD_P95_HOURS TARGET_CFR_PCT TARGET_MTTR_HOURS

          eval "$(python3 - <<'PY'
          import os

          def to_float(name: str, default: float = 0.0) -> float:
            value = os.environ.get(name, "")
            try:
              return float(value)
            except ValueError:
              return float(default)

          def status(ok: bool) -> str:
            return "âœ…" if ok else "âš ï¸"

          deploy_per_day = to_float("DEPLOY_PER_DAY", 0.0)
          lead_p95 = to_float("LEAD_P95", 0.0)
          cfr_pct = to_float("CFR_PCT", 0.0)
          mttr_hours = to_float("MTTR_HOURS", 0.0)

          target_deploy = to_float("TARGET_DEPLOY_PER_DAY", 1.0)
          target_lead = to_float("TARGET_LEAD_P95_HOURS", 24.0)
          target_cfr = to_float("TARGET_CFR_PCT", 10.0)
          target_mttr = to_float("TARGET_MTTR_HOURS", 4.0)

          print(f"DF_STATUS={status(deploy_per_day >= target_deploy)}")
          print(f"LT_STATUS={status(lead_p95 < target_lead)}")
          print(f"CFR_STATUS={status(cfr_pct < target_cfr)}")
          print(f"MTTR_STATUS={status(mttr_hours < target_mttr)}")
          PY
          )"

          {
            echo "# DORA Metrics Report"
            echo ""
            echo "**Period:** Last $PERIOD_DAYS days"
            echo "**Generated:** $(date -u +"%Y-%m-%d %H:%M UTC")"
            echo ""
            echo "## Metrics Summary"
            echo ""
            echo "| Metric | Value | Target | Status |"
            echo "|--------|-------|--------|--------|"
            echo "| Deployment Frequency | ${DEPLOY_PER_DAY}/day | >${TARGET_DEPLOY_PER_DAY}/day | $DF_STATUS |"
            echo "| Lead Time (p95) | ${LEAD_P95}h | <${TARGET_LEAD_P95_HOURS}h | $LT_STATUS |"
            echo "| Change Failure Rate | ${CFR_PCT}% | <${TARGET_CFR_PCT}% | $CFR_STATUS |"
            echo "| MTTR | ${MTTR_HOURS}h | <${TARGET_MTTR_HOURS}h | $MTTR_STATUS |"
            echo ""
            echo "## Classification"
            echo ""
            echo "**Deployment Frequency:** ${{ steps.deploy_freq.outputs.classification }}"
            echo ""
            echo "## Raw Data"
            echo ""
            echo "- Deployments counted: ${{ steps.deploy_freq.outputs.count }}"
            echo "- PRs analyzed (lead time): ${{ steps.lead_time.outputs.count }}"
            echo "- Failure PRs matched: ${{ steps.cfr.outputs.failures }}"
            echo "- Incidents analyzed (MTTR): ${{ steps.mttr.outputs.count }}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Save Metrics JSON
        env:
          DEPLOY_COUNT: ${{ steps.deploy_freq.outputs.count || '0' }}
          DEPLOY_PER_DAY: ${{ steps.deploy_freq.outputs.per_day || '0' }}
          DEPLOY_CLASSIFICATION: ${{ steps.deploy_freq.outputs.classification || 'unknown' }}
          LEAD_PRS_ANALYZED: ${{ steps.lead_time.outputs.count || '0' }}
          LEAD_MEDIAN_HOURS: ${{ steps.lead_time.outputs.median_hours || '0' }}
          LEAD_P95_HOURS: ${{ steps.lead_time.outputs.p95_hours || '0' }}
          CFR_FAILURE_PRS: ${{ steps.cfr.outputs.failures || '0' }}
          CFR_PERCENT: ${{ steps.cfr.outputs.percent || '0' }}
          MTTR_INCIDENTS_ANALYZED: ${{ steps.mttr.outputs.count || '0' }}
          MTTR_AVG_HOURS: ${{ steps.mttr.outputs.hours || '0' }}
        run: |
          set -euo pipefail

          mkdir -p metrics
          python3 - <<'PY'
          import json
          import os
          from datetime import datetime, timezone

          def to_int(value: str, default: int = 0) -> int:
            try:
              return int(float(value))
            except ValueError:
              return default

          def to_float(value: str, default: float = 0.0) -> float:
            try:
              return float(value)
            except ValueError:
              return default

          data = {
            "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
            "period_days": to_int(os.environ.get("PERIOD_DAYS", "0")),
            "metrics": {
              "deployment_frequency": {
                "count": to_int(os.environ.get("DEPLOY_COUNT", "0")),
                "per_day": to_float(os.environ.get("DEPLOY_PER_DAY", "0")),
                "classification": os.environ.get("DEPLOY_CLASSIFICATION", "unknown"),
              },
              "lead_time_hours": {
                "prs_analyzed": to_int(os.environ.get("LEAD_PRS_ANALYZED", "0")),
                "median": to_float(os.environ.get("LEAD_MEDIAN_HOURS", "0")),
                "p95": to_float(os.environ.get("LEAD_P95_HOURS", "0")),
              },
              "change_failure_rate": {
                "failure_prs": to_int(os.environ.get("CFR_FAILURE_PRS", "0")),
                "percent": to_float(os.environ.get("CFR_PERCENT", "0")),
              },
              "mttr_hours": {
                "incidents_analyzed": to_int(os.environ.get("MTTR_INCIDENTS_ANALYZED", "0")),
                "average": to_float(os.environ.get("MTTR_AVG_HOURS", "0")),
              },
            },
          }

          with open("metrics/dora-metrics.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
          PY

          cat metrics/dora-metrics.json

      - name: Upload Metrics Artifact
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0 (pinned SHA; https://github.com/actions/upload-artifact/releases/tag/v6.0.0)
        with:
          name: dora-metrics-${{ github.run_id }}
          path: metrics/
          retention-days: 365
